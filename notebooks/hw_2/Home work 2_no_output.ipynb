{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home work 2 \n",
    "\n",
    "[Dataset](https://www.kaggle.com/danielgrijalvas/movies/version/2)\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "1. Preprocessing of dataset, explaination what column do you use, why, In you skip any column explain why. Please analize usage of [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and/or [TF-IDF Vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).You allow to use external sources. \n",
    "2. K-means. Find the best param: number of clusters `n_clusters` (train muptiple models choose the best one). Show that you trained at least two k-means models (train kmeans for dataset with features received by CountVectorizer/TF-IDF Vectorizer and without).\n",
    "3. Visualize cluster for the best model (2D case, you are allowed to use t-SNE or PCA if you want, not compulsory though)\n",
    "4. DBScan. Find the best params: epsilon `eps` and minimum number of samples `min_samples`(train muptiple models choose the best one). Show that you trained at least two dbscan models (train dbscan for dataset with features received by CountVectorizer/TF-IDF Vectorizer and without)\n",
    "5. Visualize clusters for the best model (2D case, you are allowed to use t-SNE or PCA if you want, not compulsory though)\n",
    "6. Summary for both approaches (describe the model accuracy, performance, score, etc.)\n",
    "\n",
    "## Additional Info\n",
    "\n",
    "Sklearn K-means class has property - `inertia_` sum of squared distances of samples to their closest cluster center. Could be used for comparison.\n",
    "\n",
    "[Here](https://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html#sphx-glr-auto-examples-cluster-plot-mini-batch-kmeans-py) is example how you can compare two clustering approaches\n",
    "\n",
    "[Silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score)- method of interpretation and validation of consistency within clusters of data [from wiki](https://en.wikipedia.org/wiki/Silhouette_(clustering))\n",
    "\n",
    "t-SNE - method to reduce the dimensionality down to 2 dimenions, check the [kaggle kernel](https://www.kaggle.com/ffisegydd/cluster-analysis-of-movies-data) for example\n",
    "\n",
    "## Submit\n",
    "\n",
    "Two options for submition: via email or on [distedu.ukma.edu.ua](https://distedu.ukma.edu.ua/course/view.php?id=32)\n",
    "\n",
    "You should submit jupyter notebook by Sunday, November 18th till 11:55 pm EEST timezone.\n",
    "\n",
    "\n",
    "## Bonus points\n",
    "\n",
    "Hierarchical clustering. Find best params. Dendogram and explanation.\n",
    "Try all four merge strategies (complete, average, single linkages and ward criteria)\n",
    "(8 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step1: Initialization, reading input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "\n",
    "from scipy import arange\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage  \n",
    "from scipy.spatial.distance import pdist\n",
    "import scipy.cluster.hierarchy as shc\n",
    "  \n",
    "from sklearn.cluster import AgglomerativeClustering,DBSCAN,KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from plotly.offline import init_notebook_mode,iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = pd.read_csv(\"movies.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step2: Data exploration & preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nulls, everything except object-typed values seems OK.<br>\n",
    "Let's check how many unique values each category has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_data.nunique().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features have pretty low amount of unique values (genre, country, year). Others(vote, name, gross) are almost all unique.<br>\n",
    "Let's look how unique values are distributed among different categories(company/country/genre/rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"plotting+\" func\n",
    "def show_dist(series,title,threshold):\n",
    "    temp,cnt,num = pd.Series(),0,0\n",
    "    #not really effective, but the idea was to create that 'other' category\n",
    "    for i in series.index:\n",
    "        if series[i]<threshold:\n",
    "            num+=series[i]\n",
    "            cnt+=1\n",
    "        else:\n",
    "            temp[i]=series[i]\n",
    "    temp['Other'] = num\n",
    "    text = \"Index 'Other' contains \"+str(cnt)+\" indexes\"\n",
    "    plt.figure(1,figsize=(8,8))\n",
    "    plt.bar(temp.index,temp.values)\n",
    "    plt.suptitle(title+'\\n'+text)\n",
    "    #plt.xlabel(text)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    #category_name - category_description - threshold\n",
    "    #threshold is chosen manually\n",
    "    \n",
    "    [\"company\",\"Movies by company of production:\",200],\n",
    "    [\"country\",\"Movies by country of production:\",100],\n",
    "    [\"genre\",\"Movies by genre:\",100],\n",
    "    [\"rating\",\"Movies by rating:\",100]\n",
    "]\n",
    "for category in categories:\n",
    "    naming,title, threshold = category    \n",
    "    show_dist(temp_data[naming].value_counts(),title,threshold)\n",
    "#Wasted 2 hours on this\n",
    "#The next day I found same graphs in the dataset description on Kaggle. Ah, whatever.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may see, most features with relatively few categories (i.e. rating, genre) have a \"dominant category\", which sometimes consists of more elements than all other categories of that feature.<br>\n",
    "Special category is a year, because all the values there are spread evenly<br>\n",
    "Some numerical categories (especially, score, can be normalized and transformed into categories with discrete number of values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#possibly useful normalization function\n",
    "def normalize(dataframe,cols= ['budget','gross','runtime','score','votes']):\n",
    "    temp = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(dataframe[cols]))\n",
    "    dataframe[cols] = temp\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's test a function. We'll be using: budget,gross,runtime,score,votes\n",
    "#columns = ['budget','gross','runtime','score']\n",
    "test_df_normalized = normalize(temp_data)\n",
    "test_df_normalized.head()\n",
    "#seems legit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain columns should be tf-idf vectorized, others may simply be grouped by<br>\n",
    "We won't group by 'director','star','writer'.In my opinion, it makes no sense:<br>\n",
    "- firstly, I don't see any logic behind building tf-idf word matrices by person's separate name and surname. Imagine we have Tony Scott, Ridley Scott and Tony Kaye. Each of them is a director, shares name or surname with another, yet their movies are absolutely different. \n",
    "- secondly, there are simply too many unique values. Adding them all will extend our model with thousands of extra features of questionable quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, I don't see any sense to group any column by words: let's look at a distribution of words met in a column \"company\" and then analyze a column \"name\": "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_column_by_words(column,threshold=100):\n",
    "    #adding something to pd.Series ain't efficient at all, but its a good way to generate a new words table step by step\n",
    "    temp = pd.Series(temp_data[column].str.split())\n",
    "    #small list of words to ignore\n",
    "    stopwords = [\"the\",\"of\",\"and\",\"in\",\"to\",\"el\",\"la\",\"de\",\"a\",\"&\",\"on\",\"for\",\"2\",\"2:\",\"3\",\"4\",\"5\"]#I chose them myself\n",
    "    words = pd.Series()\n",
    "    for line in temp:\n",
    "        for word in line:\n",
    "            if word.lower() not in stopwords:\n",
    "                word = re.sub(\"[^a-zA-Z]\",\"\",word)\n",
    "                if word==\"\":\n",
    "                    continue\n",
    "                if word in words:\n",
    "                    words[word]+=1\n",
    "                else:\n",
    "                    words[word]=1\n",
    "    print(\"Total words:\", words.count())\n",
    "    print(\"Unique words:\",words.nunique(),\"\\n\\nWords by frequency:\\n\")\n",
    "    print(words.sort_values(ascending=False))\n",
    "    show_dist(words,\"Words by frequency\",threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_column_by_words(\"company\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most common words are \"Pictures\", \"Films\", \"Entertainment\", etc. Using tf-idf model by this words (in our task and current dataset) is senseless.<br>\n",
    "Maybe the only suitable column will be 'name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we ignored some of the most typical words and set threshold at 30\n",
    "\n",
    "explore_column_by_words(\"name\",25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering: naive approach\n",
    "Primarily, we'll assume that we don't all text fields except for 'company'\n",
    "We will generate bool features using get_dummies on 'genre' and 'rating':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing: cleaning the data set. \n",
    "temp_data = pd.read_csv(\"movies.csv\")\n",
    "#we just drop most of text values\n",
    "temp_data.released = pd.to_datetime(temp_data.released)\n",
    "temp_data.drop(['director','name','star','country','writer','released'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating dummies for groups where tf-idf/countvectorizer are pointless\n",
    "temp_data = pd.concat([temp_data,pd.get_dummies(temp_data.genre.astype('category'))],axis=1)\n",
    "temp_data = pd.concat([temp_data,pd.get_dummies(temp_data.rating.astype('category'))],axis=1)\n",
    "\n",
    "temp_data.drop(['genre','rating'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_d - set with no vectorization except for genre&rating\n",
    "data_d = temp_data.copy()\n",
    "data_d.drop('company',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of curiosity, we will use tf-idf vectorizer with 'company' (around 2.5k unique words).\n",
    "We'll create additional dataframe for that. Also, we'll create a dataframe with 'company', vectorized by count-vectorizer. Then we'll run KMeans model on different dataframes and compare results using inertia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    lowercase=True,\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    max_features=2500,\n",
    "    min_df = 1)\n",
    "\n",
    "def tfidf_vectorize_feature(data,feature):\n",
    "    v = tfidf_vectorizer.fit_transform(data[feature])\n",
    "    tfidf_vect = pd.DataFrame(v.todense(), columns=tfidf_vectorizer.get_feature_names())\n",
    "    temp = pd.concat([data, tfidf_vect], axis=1)\n",
    "    temp.drop(feature,axis=1,inplace=True)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer\n",
    "count_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    lowercase=True,\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    max_features=2500,\n",
    "    min_df = 1)\n",
    "\n",
    "def count_vectorize_feature(data,feature):\n",
    "    c = count_vectorizer.fit_transform(data[feature])\n",
    "    count_vect = pd.DataFrame(c.todense(),columns=count_vectorizer.get_feature_names())\n",
    "    temp = pd.concat([data,count_vect], axis=1)\n",
    "    temp.drop(feature,axis=1,inplace=True)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize single feature\n",
    "data_t = tfidf_vectorize_feature(temp_data,'company')\n",
    "data_c = count_vectorize_feature(temp_data,'company')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step3: Building K-Means model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Means - different num of clusters\n",
    "def kmeans_model(desc,clusters,data):\n",
    "    inertia = []\n",
    "    print(desc)\n",
    "    for i in range(2,clusters+1):\n",
    "        model = KMeans(n_clusters=i)\n",
    "        model.fit(data)\n",
    "        inertia.append(model.inertia_)\n",
    "        if i%10==0:\n",
    "            print(\"Inertia on {} clusters: {}\".format(i,model.inertia_))\n",
    "    return inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"almost no text features\" approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#runs ~40mins(i5-7300HQ), inertia on every set is very similar and giant (around 1.3*10^18 for 20 clusters)\n",
    "stats_d = kmeans_model(\"Starter, only dummies\",50,data_d)#runs in couple minutes\n",
    "stats_t = kmeans_model(\"Starter with dummies, tf-idf-ized 'company'\",50,data_t)\n",
    "stats_c = kmeans_model(\"Starter with dummief, countv-ized 'company'\",50,data_c)\n",
    "#Starter with dummies, tf-idf-ized 'company'\n",
    "#Inertia on 10 clusters: 2.7484860688811126e+18\n",
    "#Inertia on 20 clusters: 1.3325447011389642e+18\n",
    "#Inertia on 30 clusters: 8.614824405600724e+17\n",
    "#Inertia on 40 clusters: 6.403562908464607e+17\n",
    "#Inertia on 50 clusters: 5.0286967614973037e+17\n",
    "#Starter with dummief, countv-ized 'company'\n",
    "#Inertia on 10 clusters: 2.749849066197541e+18\n",
    "#Inertia on 20 clusters: 1.3330596587224942e+18\n",
    "#Inertia on 30 clusters: 8.707042343487918e+17\n",
    "#Inertia on 40 clusters: 6.274067966952358e+17\n",
    "#Inertia on 50 clusters: 4.969134127733565e+17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, performance between tf-idf and countvec doesn't really differ. It doesn't mean they're the same thing. Maybe it means that we chose features poorly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering: everything featurized*\n",
    "\n",
    "*everything but 'released'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation\n",
    "def prepare_dataset():\n",
    "    temp_data = pd.read_csv(\"movies.csv\")  \n",
    "    #temp_data.released = pd.to_datetime(temp_data.released)\n",
    "    #temp_data[['r_year','r_month','r_day']] = temp_data.released.apply(lambda x: pd.Series(x.strftime(\"%Y,%m,%d\").split(\",\")))\n",
    "\n",
    "    #getting dummies from smaller categories\n",
    "    temp_data = pd.concat([temp_data,pd.get_dummies(temp_data.genre.astype('category'))],axis=1)\n",
    "    temp_data = pd.concat([temp_data,pd.get_dummies(temp_data.rating.astype('category'))],axis=1)\n",
    "    #temp_data = pd.concat([temp_data,pd.get_dummies(temp_data.year.astype('category'))],axis=1)    \n",
    "    #temp_data = pd.concat([temp_data,pd.get_dummies(temp_data.r_year.astype('category'))],axis=1)\n",
    "    \n",
    "    temp_data.drop(columns=['released','genre','rating'],axis=1,inplace=True)\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = ['company','country','director','name','star','writer']\n",
    "data_tfd = prepare_dataset()\n",
    "\n",
    "for f in features:\n",
    "    data_tfd = tfidf_vectorize_feature(data_tfd,f)\n",
    "\n",
    "stats_all_vect = kmeans_model(\"Starter, all features vectorized\",50,data_tfd)#don't run - tons of time and shitty results\n",
    "#10 clusters: 2.748529066997636e+18\n",
    "#20 clusters: 1.3400117916318536e+18\n",
    "...\n",
    "#50 clusters: 5.044264970147402e+17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more adequate approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets repeat 2 previous approaches with normalized features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tfd_normalized = normalize(prepare_dataset)\n",
    "for f in features:\n",
    "    data_tfd_normalized = tfidf_vectorize_feature(data_tfd_normalized,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats_tfd_normalized = kmeans_model(\"Normalized set\",50,data_tfd_normalized) #runs for another 30 mins.\n",
    "#Normalized set\n",
    "#Inertia on 10 clusters: 27963.21365206909\n",
    "#Inertia on 20 clusters: 26670.18770438666\n",
    "#Inertia on 30 clusters: 25839.625499081725\n",
    "#Inertia on 40 clusters: 25523.85550936234\n",
    "#Inertia on 50 clusters: 25191.060694315413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#let's try scaler instead of our normalization\n",
    "data = prepare_dataset()\n",
    "for f in features:\n",
    "    data = tfidf_vectorize_feature(data,f)\n",
    "\n",
    "pipeline = Pipeline([('scale', StandardScaler())])\n",
    "tfd_scaled = pipeline.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stats_tfd_scaled = kmeans_model(\"StandardlyScaled set\",50,data_tfd_normalized)# almost no difference, don't run\n",
    "#10 clusters: 27823.169699275626\n",
    "#20 clusters: 26602.967411147692\n",
    "#30 clusters: 25853.884082875506\n",
    "#40 clusters: 25480.932187425915\n",
    "#50 clusters: 25220.73599407387"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let´s plot them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(12,12))\n",
    "plt.plot(stats_d,label='only dummies')\n",
    "plt.plot(stats_t,label='only one feature tf-idf-ized')\n",
    "plt.plot(stats_c,label='only one feature tf-idf-ized')\n",
    "\n",
    "plt.plot(stats_all_vect,label=\"all features vectorized\")\n",
    "plt.plot(stats_tfd_normalized,label='all features vectorized and normalized')\n",
    "plt.plot(stats_tfd_scaled,label='all features vectorized and scaled standardly')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(12,12))\n",
    "plt.plot(stats_tfd_normalized,label='all features vectorized and normalized')\n",
    "plt.plot(stats_tfd_scaled,label='all features vectorized and scaled standardly')\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more dummies, less tf-idf:\n",
    "data = prepare_dataset()\n",
    "data = pd.concat([data,pd.get_dummies(data.country.astype('category'))],axis=1)\n",
    "\n",
    "data.drop(['country','company','name','director','star','writer'],axis=1,inplace=True)\n",
    "data = normalize(data,cols=['budget','gross','runtime','score','votes','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_c_vec = kmeans_model(\"No vectorization\",50,data)#No vectorization\n",
    "#Inertia on 10 clusters: 6418.163381197919\n",
    "#Inertia on 20 clusters: 4566.427523204975\n",
    "#Inertia on 30 clusters: 3613.2555433390507\n",
    "#Inertia on 40 clusters: 3071.705614612101\n",
    "#Inertia on 50 clusters: 2655.380634499413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stats_no_vec,label=\"only ´year´,'genre' and 'rating vectorized'\")\n",
    "plt.plot(stats_c_vec,label=\"also 'company' vectorized\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via elbow method, `n_clusters` around 10 seems optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_clusters,optimal_data = 8,tfd_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: t-SNE\n",
    "\n",
    "we'll use plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_tsne(clusters,data):\n",
    "    model = KMeans(n_clusters=clusters)\n",
    "    #model.fit(data)\n",
    "    start = time.time()\n",
    "    pred = model.fit_predict(data)\n",
    "    t1 = time.time()-start #seconds to compute pred\n",
    "    print(\"model fitted in {} seconds\".format(t1))\n",
    "    tsne = TSNE()#default components num\n",
    "    start = time.time()\n",
    "    tsne_fit = tsne.fit_transform(data)\n",
    "    t2 = time.time()-start#seconds to fit-transform via t-SNE\n",
    "    print(\"tsne fitted in {} seconds\".format(t2))\n",
    "    return tsne_fit,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_notebook_mode(connected=True)\n",
    "def plot_tsne(name,fit,pred):#pred is for colors\n",
    "    \n",
    "    trace = go.Scatter(\n",
    "        x=fit.T[0], \n",
    "        y=fit.T[1],\n",
    "        mode='markers',\n",
    "        text=name,\n",
    "        textposition='top center',\n",
    "        name='Lines, Markers and Text',\n",
    "        marker=dict(\n",
    "            color = pred, \n",
    "            colorscale='Portland',\n",
    "            showscale=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    vis_data = [trace]\n",
    "    layout = go.Layout(\n",
    "    showlegend=False)\n",
    "    visualization = go.Figure(data=vis_data, layout=layout)\n",
    "    iplot(visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Non-vectorized model\". Seems to be OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tsne_fit, pred = perform_tsne(9,data)#runs around 3 mins\n",
    "plot_tsne(\"test\",tsne_fit,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_clusters,optimal_data = 12,tfd_scaled #from that graph with vectorized features above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tsne_fit, pred = perform_tsne(optimal_clusters,optimal_data)# runs about 15mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every text field splitted with tf-idf by words. Looks awful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_tsne(\"test\",tsne_fit,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: DBScan\n",
    "\n",
    "We'll find optimal `eps`, num of `samples` and compare models by `silhouette_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBScan - different number of clusters\n",
    "def dbscan_model(desc,min_eps,min_samples,data):\n",
    "    sils = []\n",
    "    print(desc)\n",
    "    for i in arange(min_eps,min_eps+5,0.2):\n",
    "        for j in arange(min_samples, min_samples+7,1):\n",
    "            print(\"eps={},min_samples={}\".format(i,j))\n",
    "            model = DBSCAN(n_jobs=-1,eps=min_eps+i,min_samples=min_samples+j)\n",
    "            model = model.fit(data)\n",
    "            pred = model.fit_predict(data)\n",
    "            if len(np.unique(model.labels_))>1:\n",
    "                sil = silhouette_score(data,model.labels_)\n",
    "                sils.append(sil)\n",
    "                print(\"Silhouette score on eps={}, min_samples={} is: {}\".format(i,j,sil))\n",
    "            #if pred[np.argmin(pred)]!=-1:\n",
    "            #if i%1==0:\n",
    "            i = round(i,1)\n",
    "            j = round(j,1)\n",
    "    return sils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#simple no-vect dataset\n",
    "data = prepare_dataset()\n",
    "data.drop(columns=['company','country','director','name','star','writer'],axis=1,inplace=True)\n",
    "data=normalize(data)\n",
    "data=normalize(data,cols=['year'])\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dbscan_novec = dbscan_model(\"No vect model:\",0.5,4,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect dataset:\n",
    "data = prepare_dataset()\n",
    "#let's tf-idf 'company','country'\n",
    "data = tfidf_vectorize_feature(data,'country')\n",
    "data = tfidf_vectorize_feature(data,'company')\n",
    "\n",
    "data=normalize(data)\n",
    "\n",
    "data=normalize(data,'year')\n",
    "data.drop(columns=['director','name','star','writer'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats_dbscan_vec = dbscan_model(\"More vect model:\",0.5,4,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(stats_dbscan_novec,label = \"non-vectorized set\")\n",
    "plt.plot(stats_dbscan_vec,label = \"vectorized set\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_eps,optimal_samples=0.7,6 #let's say that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: t-SNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_tsne_dbscan(ep,samples):\n",
    "    model = DBSCAN(n_jobs=-1,eps=ep,min_samples=samples)\n",
    "    start = time.time()\n",
    "    pred = model.fit_predict(data)\n",
    "    t1 = time.time()-start #seconds to compute pred\n",
    "    print(\"model fitted in {} seconds\".format(t1))\n",
    "    tsne = TSNE()#default components num\n",
    "    start = time.time()\n",
    "    tsne_fit = tsne.fit_transform(data)\n",
    "    t2 = time.time()-start#seconds to fit-transform via t-SNE\n",
    "    print(\"tsne fitted in {} seconds\".format(t2))\n",
    "    return tsne_fit,pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_fit,pred = perform_tsne_dbscan(optimal_eps,optimal_samples)\n",
    "plot_tsne(\"non-vectorized\",tsne_fit,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a dendogram hardly relying on rating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#preparing a moderate set of features:\n",
    "data=pd.read_csv(\"movies.csv\")\n",
    "data = normalize(data)\n",
    "data = normalize(data,cols=['year'])\n",
    "data = pd.concat([data,pd.get_dummies(data.rating.astype('category'))],axis=1)\n",
    "data.drop(['company','country','director','star','name','writer','genre','rating','released'],inplace=True,axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Z = linkage(pdist(data.T.values))#single-linkage using scipy.cluster.hierarchy\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "dendrogram(orientation='top',Z=Z,distance_sort='descending',show_leaf_counts=True,labels = data.columns)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_tsne_hierarchical(clusters):\n",
    "    cluster = AgglomerativeClustering(n_clusters=clusters, affinity='euclidean', linkage='average')  \n",
    "    start = time.time()\n",
    "    pred = cluster.fit_predict(data)  \n",
    "    t1 = time.time()-start #seconds to compute pred\n",
    "    print(\"model fitted in {} seconds\".format(t1))\n",
    "    tsne = TSNE()#default components num\n",
    "    start = time.time()\n",
    "    tsne_fit = tsne.fit_transform(data)\n",
    "    t2 = time.time()-start#seconds to fit-transform via t-SNE\n",
    "    print(\"tsne fitted in {} seconds\".format(t2))\n",
    "    return tsne_fit,pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, it's not that bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_fit,pred = perform_tsne_hierarchical(4)\n",
    "plot_tsne(\"Hierarchical\",tsne_fit,pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "### 1. K-Means\n",
    "Using sets with different amounts of vectorized features (using both tf-idf and countvectorizer) I showed that tf-idf word vectorizing fields containing lexemes is simply harmful and doesn't lead anywhere. I guess, we should cautiously use text fields from dataset in a more complex model.<br>\n",
    "Anyway, for different sets of features I built clusters\\inertia graphs and chose optimal cluster number using elbow method<br>\n",
    "I didn't get a good clusterer on \"vectorized\" version, it may be seen on a graph.<br>\n",
    "### 2. DBScan\n",
    "I tried to find optimal eps and min_samples, but got a little confused.<br>\n",
    "I don't like any of the graphs. Maybe it's about poor feature selection<br>.\n",
    "### General\n",
    "I managed to try different techniques and approaches.\n",
    "In both models I should try selecting features\\dummies differently, as well as better search for optimal`eps`,`min_samples`,etc.I guess Hierarchical clustering fits this task better than traditional K-Means.\n",
    "#### Thanks for the attention!\n",
    " \n",
    " \n",
    " \n",
    "Following notebook belongs to Alexandr Kroshyn. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
