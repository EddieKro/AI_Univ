{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport torch\nimport torchtext\nimport random\nfrom torch import nn\nfrom sklearn.metrics import f1_score\nfrom nltk import word_tokenize\nfrom torch import optim\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"random_state = random.getstate()\nbatch_size = 192","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2b66c4112261a46256cbb8975c08b31e7971b71"},"cell_type":"code","source":"text = torchtext.data.Field(lower=True, batch_first=True, tokenize=word_tokenize)\nqid = torchtext.data.Field()\ntarget = torchtext.data.Field(sequential=False, use_vocab=False, is_target=True)\ntrain = torchtext.data.TabularDataset(path='../input/train.csv', format='csv',\n                                      fields={'question_text': ('text',text),\n                                              'target': ('target',target)})\ntest = torchtext.data.TabularDataset(path='../input/test.csv', format='csv',\n                                     fields={'qid': ('qid', qid),\n                                             'question_text': ('text', text)})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"831e3d5428ea2a6d06a782d4538dbbcf8f868f09","scrolled":true},"cell_type":"code","source":"text.build_vocab(train, test, min_freq=2)\nqid.build_vocab(test)\ntext.vocab.load_vectors(torchtext.vocab.Vectors('../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'))\nprint(text.vocab.vectors.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9be0cd6ac0f83a0dea945432ba7ea8ae3ae6b1c9"},"cell_type":"code","source":"train, val = train.split(split_ratio=0.8, random_state=random_state)\n\n\ntrain_iter = torchtext.data.BucketIterator(dataset=train,\n                                           batch_size=batch_size,\n                                           sort_key=lambda x: x.text.__len__(),\n                                           shuffle=True,\n                                           sort=False)\n\nval_iter = torchtext.data.BucketIterator(dataset=val,\n                                         batch_size=batch_size,\n                                         sort_key=lambda x: x.text.__len__(),\n                                         train=False,\n                                         sort=False)\ntest_iter = torchtext.data.BucketIterator(dataset=test,\n                                           batch_size=batch_size,\n                                           sort_key=lambda x: x.text.__len__(),\n                                           sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4c9e4e52a11f1687c09a48f152a6b742cfb628c"},"cell_type":"code","source":"def training(epoch, model, loss_func, optimizer, train_iter, val_iter):\n    step = 0\n    train_record = []\n    losses = []\n    val_record = []\n    \n    for e in range(epoch):\n        train_iter.init_epoch()\n        for train_batch in iter(train_iter):\n            step += 1\n            model.train()\n            x = train_batch.text.cuda()\n            y = train_batch.target.type(torch.Tensor).cuda()\n            model.zero_grad()\n            pred = model.forward(x).view(-1)\n            loss = loss_function(pred, y)\n            loss_data = loss.cpu().data.numpy()\n            train_record.append(loss_data)\n            loss.backward()\n            optimizer.step()\n            if step % 1000 == 0:\n                print(\"Step: {:06}, loss {:.4f}\".format(step, loss_data))\n            if step % 10000 == 0:\n                model.eval()\n                model.zero_grad()\n                val_loss = []\n                for val_batch in iter(val_iter):\n                    val_x = val_batch.text.cuda()\n                    val_y = val_batch.target.type(torch.Tensor).cuda()\n                    val_pred = model.forward(val_x).view(-1)\n                    val_loss.append(loss_function(val_pred, val_y).cpu().data.numpy())\n                val_record.append({'step': step, 'loss': np.mean(val_loss)})\n                print('Epoch {:02} - step {:06} - train_loss {:.4f} - val_loss {:.4f} '.format(\n                            e, step, np.mean(train_record), val_record[-1]['loss']))\n                train_record = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fbeb4e99048aa1f8b598719e5326e27cff9ef30"},"cell_type":"code","source":"class SimpleModel(nn.Module):\n    def __init__(self, pretrained_lm, padding_idx, hidden_dim, static=True,lstm_layer=3, dropout=0.25):\n        super(SimpleModel, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.dropout = nn.Dropout(p=dropout)\n        self.embedding = nn.Embedding.from_pretrained(pretrained_lm)\n        self.embedding.padding_idx = padding_idx\n        if static:\n            self.embedding.weight.requires_grad = False\n        self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n                            hidden_size=hidden_dim,\n                            num_layers=lstm_layer, \n                            dropout = dropout,\n                            bidirectional=True)\n        self.hidden2label = nn.Linear(hidden_dim*lstm_layer*2, 1)\n        \n    def forward(self, sents):\n        x = self.embedding(sents)\n        x = torch.transpose(x, dim0=1, dim1=0)\n        lstm_out, (h_n, c_n) = self.lstm(x)\n        y = self.hidden2label(self.dropout(torch.cat([c_n[i,:, :] for i in range(c_n.shape[0])], dim=1)))\n        return y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7c157ed626f70bf9efc15f0d64e7a002f381dc4"},"cell_type":"code","source":"model = SimpleModel(text.vocab.vectors,\n                    padding_idx=text.vocab.stoi[text.pad_token],\n                    hidden_dim=64).cuda()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ff53761515c855e226e131949f3546f9f085d2e"},"cell_type":"code","source":"loss_function = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d309792c7ffe18158805e9c732d8a6db659f729e","scrolled":false},"cell_type":"code","source":"training(model=model,\n         epoch=8,\n         loss_func=loss_function,\n         optimizer=optimizer,\n         train_iter=train_iter,\n         val_iter=val_iter\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba0284ee50b9efa3ed4fae03b1ef259cefdbce81"},"cell_type":"code","source":"model.eval()\n\nval_pred = []\nval_true = []\nval_iter.init_epoch()\nfor val_batch in iter(val_iter):\n    val_x = val_batch.text.cuda()\n    val_true += val_batch.target.data.numpy().tolist()\n    val_pred += torch.sigmoid(model.forward(val_x).view(-1)).cpu().data.numpy().tolist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bd041404ee72dc531e48960ec6a7b0bb958b1f7"},"cell_type":"code","source":"tmp = [0,0,0] #idx,curr,max\ndelta = 0\nfor tmp[0] in np.arange(0.1,0.501,0.01):\n    tmp[1] = f1_score(val_true,np.array(val_pred)>tmp[0])\n    if tmp[1] > tmp[2]:\n        delta = tmp[0]\n        tmp[2] = tmp[1]\nprint(\"best threshold {:.3f} with F1-score {:.3f}\".format(delta,tmp[2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba0284ee50b9efa3ed4fae03b1ef259cefdbce81"},"cell_type":"code","source":"model.eval()\nmodel.zero_grad()\n\ntest_iter = torchtext.data.BucketIterator(dataset=test,\n                                           batch_size=batch_size,\n                                           sort_key=lambda x: x.text.__len__(),\n                                           #shuffle=True,\n                                           sort=True)\n\ntest_pred = []\ntest_id = []\n\nfor test_batch in iter(test_iter):\n    test_x = test_batch.text.cuda()\n    test_pred += torch.sigmoid(model.forward(test_x).view(-1)).cpu().data.numpy().tolist()\n    test_id += test_batch.qid.view(-1).data.numpy().tolist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdc50142388f0ff517450f25d0e6135a956144d4"},"cell_type":"code","source":"res = pd.DataFrame()\nres['qid'] = [qid.vocab.itos[i] for i in test_id]\nres['prediction'] = (np.array(test_pred) >= delta).astype(int)\nres.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}